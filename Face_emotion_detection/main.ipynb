{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt\n",
    "# pip install -U scikit-learn\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,Dropout,Flatten,MaxPooling2D\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add directory for both train & test set:\n",
    "TRAIN_FOLDER='./images/train'\n",
    "TEST_FOLDER='./images/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame(directory):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "    # treat each folder like angry,sad as a label:\n",
    "    for label in os.listdir(directory):\n",
    "        for imageName in os.listdir(os.path.join(directory,label)):\n",
    "            images.append(os.path.join(directory,label,imageName))\n",
    "            labels.append(label)\n",
    "        print(f\"{label} completed\")\n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "train=pd.DataFrame()\n",
    "train['image'],train['label']=createDataFrame(TRAIN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  image     label\n",
      "0            ./images/train\\angry\\0.jpg     angry\n",
      "1            ./images/train\\angry\\1.jpg     angry\n",
      "2           ./images/train\\angry\\10.jpg     angry\n",
      "3        ./images/train\\angry\\10002.jpg     angry\n",
      "4        ./images/train\\angry\\10016.jpg     angry\n",
      "...                                 ...       ...\n",
      "28816  ./images/train\\surprise\\9969.jpg  surprise\n",
      "28817  ./images/train\\surprise\\9985.jpg  surprise\n",
      "28818  ./images/train\\surprise\\9990.jpg  surprise\n",
      "28819  ./images/train\\surprise\\9992.jpg  surprise\n",
      "28820  ./images/train\\surprise\\9996.jpg  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# check your dataframe:\n",
    "print(train)\n",
    "# print(train['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "test=pd.DataFrame()\n",
    "test['image'],test['label']=createDataFrame(TEST_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      image     label\n",
      "0       ./images/validation\\angry\\10052.jpg     angry\n",
      "1       ./images/validation\\angry\\10065.jpg     angry\n",
      "2       ./images/validation\\angry\\10079.jpg     angry\n",
      "3       ./images/validation\\angry\\10095.jpg     angry\n",
      "4       ./images/validation\\angry\\10121.jpg     angry\n",
      "...                                     ...       ...\n",
      "7061  ./images/validation\\surprise\\9806.jpg  surprise\n",
      "7062  ./images/validation\\surprise\\9830.jpg  surprise\n",
      "7063  ./images/validation\\surprise\\9853.jpg  surprise\n",
      "7064  ./images/validation\\surprise\\9878.jpg  surprise\n",
      "7065   ./images/validation\\surprise\\993.jpg  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# check your dataframe:\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up each image & extract features:\n",
    "def extractFeatures(images):\n",
    "    features=[]\n",
    "    for image in tqdm(images):      #tqdm:for fetching each image one by one\n",
    "        img=load_img(image,color_mode='grayscale')\n",
    "        img=np.array(img)\n",
    "        features.append(img)\n",
    "    features=np.array(features)\n",
    "    features=features.reshape(len(features),48,48,1)    #size of each image is 48*48 and 2D images so depth=1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e14852c9eb421a8c61017aff46e686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features=extractFeatures(train['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706da967505742deb2fcdc7fd1c0dc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_features=extractFeatures(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_features/255.0    #dividing by highest pixel value ie 255\n",
    "x_test=test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised learning: providing labels:\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LabelEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le=LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=le.transform(train['label'])\n",
    "y_test=le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 7 labels: angry,fear,disgust,happy,neutral,sad,surprise\n",
    "y_train=to_categorical(y_train,num_classes=7)\n",
    "y_test=to_categorical(y_test,num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create object of sequential:\n",
    "model=Sequential()\n",
    "# add all convolutional layers in the model:\n",
    "model.add(Conv2D(128,kernel_size=(3,3),activation='relu',input_shape=(48,48,1)))    #shape 48x48 and depth=1 for 2D image\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256,kernel_size=(3,3),activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))   \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(7,activation='softmax'))       #we have 7 labels so output class is 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 1s/step - accuracy: 0.5619 - loss: 1.1461 - val_accuracy: 0.5896 - val_loss: 1.0795\n",
      "Epoch 2/20\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 1s/step - accuracy: 0.5620 - loss: 1.1557 - val_accuracy: 0.5940 - val_loss: 1.0907\n",
      "Epoch 3/20\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2309s\u001b[0m 10s/step - accuracy: 0.5675 - loss: 1.1395 - val_accuracy: 0.5945 - val_loss: 1.0815\n",
      "Epoch 4/20\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 892ms/step - accuracy: 0.5713 - loss: 1.1302 - val_accuracy: 0.5945 - val_loss: 1.0850\n",
      "Epoch 5/20\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 954ms/step - accuracy: 0.5655 - loss: 1.1427 - val_accuracy: 0.5967 - val_loss: 1.0839\n",
      "Epoch 6/20\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 931ms/step - accuracy: 0.5682 - loss: 1.1327 - val_accuracy: 0.5960 - val_loss: 1.0795\n",
      "Epoch 7/20\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911ms/step - accuracy: 0.5761 - loss: 1.1181"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# can stop when accuracy>75 ie around 0.75\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:395\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    386\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    387\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    394\u001b[0m     )\n\u001b[1;32m--> 395\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    407\u001b[0m }\n\u001b[0;32m    408\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:484\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    483\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m--> 484\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\patha\\OneDrive\\Desktop\\thapar\\Python\\VSCode\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test))\n",
    "# can stop when accuracy>75 ie around 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_json=model.to_json()\n",
    "with open(\"emotionDetector.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotionDetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file=open(\"emotionDetector.json\",\"r\")\n",
    "model_json=json_file.read()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_from_json(model_json)\n",
    "model.load_weights(\"emotionDetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=['angry','disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract feature from a single image\n",
    "def feature_extract(image):\n",
    "    image=load_img(image,color_mode='grayscale')\n",
    "    feature=np.array(image)\n",
    "    feature=feature.reshape(1,48,48,1)\n",
    "    return feature/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "#ie do not open another window & show here itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original happy 24 image\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Model predicted: happy\n"
     ]
    }
   ],
   "source": [
    "image='./images/train/happy/24.jpg'\n",
    "print(\"Original happy 24 image\")\n",
    "img=feature_extract(image)\n",
    "pred=model.predict(img)\n",
    "pred_label=label[pred.argmax()]\n",
    "print(f\"Model predicted: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1749b000b00>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.reshape(48,48),cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
